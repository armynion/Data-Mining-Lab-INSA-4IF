{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4IF Data Mining Lab\n",
    "Nadine Saadalla, Noémie Varjabedian, Eléonore Dravet  \n",
    "February 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "During this Data Mining project, we will be analyzing the geolocation data of photographs taken in the Grand Lyon. The data comes from the Flickr database and was taken between 2010 and 2019. We will explore different clustering methods to identify areas of interest for tourism in the metropolis, i.e. areas where many photographs are taken. Finally, we'll work on photograph captions to assign descriptions to our areas of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python set-up\n",
    "\n",
    "In this section, we will set up a virtual python environment and install all necessary packages to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ./vitualenvpython/bin/activate\n",
    "! which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of required libraries and dependencies\n",
    "# numeric calculations\n",
    "! pip install numpy==1.26.0 \n",
    "# data frames \n",
    "! pip install pandas==2.1.1 \n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn==1.5.1 \n",
    "! pip install scipy==1.12.0\n",
    "# plotting \n",
    "! pip install plotly==5.24.1 \n",
    "! pip install matplotlib==3.8.0 \n",
    "! pip install plotly-express==0.4.1 \n",
    "! pip install chart-studio==1.1.0 \n",
    "# web app library \n",
    "! pip install streamlit==1.37.1 \n",
    "#maping library\n",
    "! pip install folium\n",
    "# association rules\n",
    "! pip install mlxtend==0.23.3\n",
    "\n",
    "! pip install nbformat==5.9.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas to deal with the data\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import numpy as np\n",
    "#Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "#Map\n",
    "import folium\n",
    "#clustering groups\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "#Hierarchical\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#Tags\n",
    "from collections import Counter\n",
    "#Images\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and cleaning\n",
    "\n",
    "We start by viewing the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"flickr_data2.csv\", sep=\",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data in unnamed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the three lines    \n",
    "\n",
    "16  Unnamed: 16          142 non-null     float64  \n",
    "17  Unnamed: 17          0 non-null       float64  \n",
    "18  Unnamed: 18          2 non-null       float64  \n",
    "\n",
    "which indicate that some lines have values in unnamed columns, probably due to a wrong use of the separators ','. They represent a very small ration of the overall data (~150/420 000) so we chose to delete those lines without further investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_columns = data.columns[data.columns.str.contains('^Unnamed')]\n",
    "data = data.loc[~data[\"Unnamed: 16\"].notna(),:]\n",
    "data = data.loc[~data[\"Unnamed: 17\"].notna(),:]\n",
    "data = data.loc[~data[\"Unnamed: 18\"].notna(),:]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines have been deleted (\"0 non-null\"). The columns still appear, we can delete them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=unnamed_columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that except for 'id', the column names start with a space. To manipulate the columns more easily, we remove these spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.rename(columns=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoherent values\n",
    "We can look at the statistics to check that they are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the max date_taken_year is 2238, which is impossible. This pushes us to delete all rows where the date taken is more than 2025 (the current year), and print all the years where pictures have been taken in our dataset, to check that everything is coherent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=data[data['date_taken_year']>2025].index\n",
    "data.drop(index=index,axis=1)\n",
    "taken_years = data[\"date_taken_year\"].unique()\n",
    "uploded_years = data[\"date_upload_year\"].unique()\n",
    "print(f\" When pictures are taken : {taken_years}\\n\")\n",
    "print(f\" When pictures are uploaded : {uploded_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"missing-vals\"></a>\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the missing values are either from the tags, the title or the upload_year. For our analysis on the coordinates, these information are not determining and we can choose to keep the corresponding data. However, in order to run our algorithm, we sample the data (our CPUs are not powerful enough to run with all the data) and keep only a thousand points, so we may as well keep the data with tags for our text analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(index=data[data['tags'].isna()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"duplicates\"></a>\n",
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis to be pertinent, we do not want to count several times pictures taken by the same person, in the same place, in the same hour. This allows us to count visits. This avoids getting influenced by someone taking burst photos as well. Hence we remove the duplicates using the following columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_duplicates=['user', 'lat', 'long','date_taken_hour', 'date_taken_day', 'date_taken_month','date_taken_year', 'date_upload_minute', 'date_upload_hour','date_upload_day', 'date_upload_month', 'date_upload_year']\n",
    "data=data.drop_duplicates(subset=columns_for_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have ~90 00 points from the ~420 000 initial. We still have enough values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize all the points on a map as a last check of our values. We choose 1000 random tagged points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map([45.762611,4.832805\t], zoom_start=14)\n",
    "data_sample = data.sample(1000)\n",
    "\n",
    "for index, row in data_sample.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"],row[\"long\"]],\n",
    "        icon=folium.DivIcon(html=f\"\"\"<svg width=\"20px\" height=\"20px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "        <path d=\"M12 9.5C13.3807 9.5 14.5 10.6193 14.5 12C14.5 13.3807 13.3807 14.5 12 14.5C10.6193 14.5 9.5 13.3807 9.5 12C9.5 10.6193 10.6193 9.5 12 9.5Z\" fill=\"#e63946\"/>\n",
    "        </svg>\"\"\")\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks cleaned. We can save our cleaned data to a new csv and work with this version as for now. We will not have do do the treatment again since we can just directly use the data_cleaned.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./data_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing clsutering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cleaned=pd.read_csv('./data_cleaned.csv')\n",
    "data_sample = data_cleaned.sample(1000, random_state=42)\n",
    "cluster_colors = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF', '#FFA500', '#800080']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering and Find the Optimal Number of Clusters using Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the Elbow method, we observe the different clusters we obtain by changing the value of k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method \n",
    "We use the elbow method to determine the best parameter k for our K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for i in range(1,30):\n",
    "  kmeans=KMeans(n_clusters=i,init='k-means++')\n",
    "  kmeans.fit(data_sample[[\"lat\", \"long\"]])\n",
    "  inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,30),inertias)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value seems to be k=7. When we visualize the result on the map, there are not enough clusters to represent touristic places. We hence chose to use a higher k to adapt to the reality.\n",
    "\n",
    "P.S. : We only show clusters than contain more than 10 points. Samller clusters are considered irrelevant. This changes nothing for k=7 but for a bigger k or for other methods, this rule has an impact on the printed map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means = 7 | 100 maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "kmeans.fit(data_sample[[\"lat\", \"long\"]])\n",
    "\n",
    "m = folium.Map(location=[45.762611, 4.832805], zoom_start=14)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "clustered_points = data_sample.assign(cluster=labels)\n",
    "\n",
    "for cluster_id in clustered_points['cluster'].unique():\n",
    "    cluster_points = clustered_points[clustered_points['cluster'] == cluster_id]\n",
    "    points = cluster_points[['lat', 'long']].values\n",
    "\n",
    "    if len(points) >= 10:  # We only show clusters with more than 10 points.\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]  \n",
    "            \n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=cluster_colors[cluster_id % len(cluster_colors)],\n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Cluster {cluster_id}: QhullError occurred; skipping hull computation.\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}: Less than 10 points; skipping perimeter.\")\n",
    "\n",
    "for _, row in data_sample.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"long\"]],\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "        <svg width=\"10px\" height=\"10px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <circle cx=\"12\" cy=\"12\" r=\"5\" fill=\"black\"/>\n",
    "        </svg>\n",
    "        \"\"\")\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"cluster_map_k_means_7.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "kmeans.fit(data_sample[[\"lat\", \"long\"]])\n",
    "\n",
    "m = folium.Map(location=[45.762611, 4.832805], zoom_start=14)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "clustered_points = data_sample.assign(cluster=labels)\n",
    "\n",
    "for cluster_id in clustered_points['cluster'].unique():\n",
    "    cluster_points = clustered_points[clustered_points['cluster'] == cluster_id]\n",
    "    points = cluster_points[['lat', 'long']].values\n",
    "\n",
    "    if len(points) >= 10:  # We only show clusters with more than 10 points.\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]  \n",
    "            \n",
    "            \n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=cluster_colors[cluster_id % len(cluster_colors)],\n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Cluster {cluster_id}: QhullError occurred; skipping hull computation.\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}: Less than 10 points; skipping perimeter.\")\n",
    "\n",
    "for _, row in data_sample.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"long\"]],\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "        <svg width=\"10px\" height=\"10px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <circle cx=\"12\" cy=\"12\" r=\"5\" fill=\"black\"/>\n",
    "        </svg>\n",
    "        \"\"\")\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"cluster_map_k_means_100.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #5: Cluster Evaluation using Silhouette Coefficient | A FAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the clustering, we can use **Silhouette Coefficient**. The Silhouette Coefficient for a sample is given by $(b - a) / max(a, b)$ where `b` is the distance between a sample and the nearest cluster that the sample is not a part of, and `a` is the mean intra-cluster distance (i.e. the mean distance between a sample and all other samples in the same cluster). \n",
    "\n",
    "The silhouette score ranges from -1 to 1 and indicates how well each data point fits within its assigned cluster:\n",
    "\n",
    "* Score near +1 means:\n",
    "    - The data point is far from neighboring clusters\n",
    "    - The point is well-matched to its cluster\n",
    "    - Indicates very distinct, well-separated clustering\n",
    "* Score near 0 means:\n",
    "    - The data point is close to the decision boundary between clusters\n",
    "    - The point could potentially belong to either cluster\n",
    "    - Suggests overlapping or not well-defined clusters\n",
    "* Score near -1 means:\n",
    "    - The data point might be assigned to the wrong cluster\n",
    "    - The point is closer to points in another cluster than its own\n",
    "    - Indicates poor clustering or potential misassignments\n",
    "\n",
    "We can use [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.silhouette_score.html) and [`sklearn.metrics.silhouette_samples`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "* For k-means clustering with `k=3`, calculate Silhouette score for each data point, for each cluster and average silhouette score \n",
    "* Display Silhouette score plot\n",
    "* Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=3,init='k-means++')\n",
    "for_silhouette_df = scaled_data_df.copy()\n",
    "for_silhouette_df[\"labels\"] = labels\n",
    "\n",
    "for_silhouette_df = for_silhouette_df.sort_values(by=[\"labels\"])\n",
    "\n",
    "for_silhouette_df[\"silhouettes\"] = silhouette_samples(for_silhouette_df[features],for_silhouette_df[\"labels\"])\n",
    "\n",
    "cluster_silhouette_score = for_silhouette_df.groupby([\"labels\"]).mean()[\"silhouettes\"]\n",
    "\n",
    "average_silhouette_score = for_silhouette_df[\"silhouettes\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,20))\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "y_lower = 10\n",
    "for i in range(3):\n",
    "    ith_cluster_silhouette_values = for_silhouette_df.loc[for_silhouette_df[\"labels\"] == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort_values(by=[\"silhouettes\"])\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / 3)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "ax1.axvline(x=average_silhouette_score, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([]) \n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As general guidelines, the plot can be interpreted by looking at:\n",
    "* *the thickness of the clusters (number of points)*;\n",
    "* check if any cluster has many negative values;\n",
    "* check the consistency of the silhouette widths within clusters;\n",
    "* the average value. Recall that in general, the following interpretation applies:\n",
    "    - \\> 0.7: Strong clustering structure\n",
    "    - 0.5 - 0.7: Reasonable clustering structure\n",
    "    - 0.25 - 0.5: Weak clustering structure\n",
    "    - < 0.25: No substantial clustering structure\n",
    "\n",
    "\n",
    "* Cluster Silhouette scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR COMMENT: TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering : single, complete and ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we analyse the 3 variations of hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_single = AgglomerativeClustering(n_clusters=100, linkage='single').fit(data_sample[[\"lat\", \"long\"]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_complete = AgglomerativeClustering(n_clusters=100, linkage='complete').fit(data_sample[[\"lat\", \"long\"]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_ward = AgglomerativeClustering(n_clusters=100, linkage='ward').fit(data_sample[[\"lat\", \"long\"]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing on the map the different results, we determine that the best hierarchical cluserting method is the ward one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the results on the map, we only need to change this line below with the corresponding clustering variation and then run the code. We left it at the ward method because we have concluded that it's the best of the three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clustering_ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[45.762611, 4.832805], zoom_start=14)\n",
    "clustered_points = data_sample.assign(cluster=labels)\n",
    "\n",
    "for cluster_id in clustered_points['cluster'].unique():\n",
    "    cluster_points = clustered_points[clustered_points['cluster'] == cluster_id]\n",
    "    points = cluster_points[['lat', 'long']].values\n",
    "\n",
    "    if len(points) >= 10: # We only show clusters with more than 10 points.\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]\n",
    "            \n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=cluster_colors[cluster_id % len(cluster_colors)], \n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2,\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Cluster {cluster_id}: QhullError occurred; skipping hull computation.\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}: Less than 10 points; skipping perimeter.\")\n",
    "\n",
    "for _, row in data_sample.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"long\"]],\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "        <svg width=\"10px\" height=\"10px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <circle cx=\"12\" cy=\"12\" r=\"5\" fill=\"black\"/>\n",
    "        </svg>\n",
    "        \"\"\"),\n",
    "\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"cluster_map_hierarchical.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Apply DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "* Apply [sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) algorithm\n",
    "* Identify the best values for `eps` and `min_sanples` by varying the values within a range and by using Silhouette coefficient\n",
    "* Apply DBSCAN with the best parameters found\n",
    "* Print number of clusters and noise points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "data_sample = data_sample.sample(1000, random_state=42)\n",
    "\n",
    "clustering_db = DBSCAN(eps=0.002, min_samples=3).fit(data_sample[[\"lat\", \"long\"]] )\n",
    "clustering_db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull, QhullError\n",
    "\n",
    "# Initialize the map\n",
    "m = folium.Map(location=[45.762611, 4.832805], zoom_start=12)\n",
    "\n",
    "# Assign cluster labels\n",
    "labels = clustering_db.labels_\n",
    "\n",
    "# Group points by cluster\n",
    "clustered_points = data_sample.assign(cluster=labels)\n",
    "\n",
    "# Define colors for clusters (extendable)\n",
    "cluster_colors = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF', '#FFA500', '#800080']\n",
    "\n",
    "# Track if any polygons are added\n",
    "polygon_added = False\n",
    "\n",
    "# Loop through each cluster and compute its perimeter\n",
    "for cluster_id in clustered_points['cluster'].unique():\n",
    "\n",
    "    if cluster_id < 0:\n",
    "        print(f\"Skipping cluster {cluster_id} (outlier)\")\n",
    "        continue\n",
    "\n",
    "    cluster_points = clustered_points[clustered_points['cluster'] == cluster_id]\n",
    "    points = cluster_points[['lat', 'long']].values  # Extract points as (lat, long)\n",
    "\n",
    "    if len(points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]  # Get the hull points\n",
    "            \n",
    "            # Add the perimeter as a polygon to the map\n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=cluster_colors[cluster_id % len(cluster_colors)],  # Use modulo for repeating colors\n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(m)\n",
    "            polygon_added = True  # Mark that a polygon was successfully added\n",
    "        except QhullError:\n",
    "            print(f\"Cluster {cluster_id}: QhullError occurred; skipping hull computation.\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}: Less than 3 points; skipping perimeter.\")\n",
    "\n",
    "# Add markers for all points\n",
    "for _, row in data_sample.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"long\"]],\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "        <svg width=\"10px\" height=\"10px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <circle cx=\"12\" cy=\"12\" r=\"5\" fill=\"black\"/>\n",
    "        </svg>\n",
    "        \"\"\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save and display the map\n",
    "m.save(\"cluster_map_dbscan.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Cluster Characterisation using Apriori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to describe the obtained cluster. To do so, let's use frequent pattern mining and in particular **Apriori algorithm**. \n",
    "\n",
    "**QUESTIONS**\n",
    "* First, convert numerical features to categorical (low, medium, high) based on quantiles. Add binary columns, e.g. `sepal length low`, `sepal length medium`, `sepal length high` depending on the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find association, we are going to use [mlxtend.frequent_patterns.apriori](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent patterns\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "* Use apriori algorithm to find frequent patterns for each cluster\n",
    "* Then among these itemsets, find those that are not frequent for other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special function : show image on click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show image taken when we click on a point, we had to first get a flicker key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLICKR_API_KEY = \"c9f9183c6f9f862b589a12aa27b9c8e6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of showing images :\n",
    "- We first wanted to show images on hover but the only way to do that was to preload all the images and that approximately 27 seconds to load 100 images and 5min30s to load all 1000.\n",
    "- Thus we decided to show images on click instead however we faced the issue of dynamically rendering the photos when we click on the marker.\n",
    "- We used HTML in order to dynamically render the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable_image_path = os.path.abspath(\"unavailable-image.jpg\")\n",
    "\n",
    "fetch_image_script = f\"\"\"\n",
    "<script>\n",
    "function fetchFlickrImage(photo_id, containerId) {{\n",
    "    console.log(\"Fetching image for photo_id:\", photo_id);\n",
    "    var apiUrl = \"https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key={FLICKR_API_KEY}&photo_id=\" + photo_id + \"&format=json&nojsoncallback=1\";\n",
    "\n",
    "    fetch(apiUrl)\n",
    "        .then(response => response.json())\n",
    "        .then(data => {{\n",
    "            var container = document.getElementById(containerId);\n",
    "\n",
    "            if (data.photo) {{\n",
    "                var server_id = data.photo.server;\n",
    "                var secret = data.photo.secret;\n",
    "                var imageUrl = \"https://live.staticflickr.com/\" + server_id + \"/\" + photo_id + \"_\" + secret + \".jpg\";\n",
    "                console.log(\"Image URL:\", imageUrl);\n",
    "\n",
    "                // Replace placeholder with the fetched image\n",
    "                container.innerHTML = \"<img src='\" + imageUrl + \"' style='width: 250px; height: 180px; object-fit: cover; border-radius: 8px; display: block; margin: auto;'/>\";\n",
    "            }} else {{\n",
    "                console.log(\"Image not found, using fallback.\");\n",
    "                container.innerHTML = \"<img src='file://{unavailable_image_path}' style='width: 250px; height: 180px; object-fit: cover; border-radius: 8px; display: block; margin: auto;'/>\";\n",
    "            }}\n",
    "        }})\n",
    "        .catch(error => {{\n",
    "            console.error(\"Error fetching image:\", error);\n",
    "            var container = document.getElementById(containerId);\n",
    "            container.innerHTML = \"<img src='file://{unavailable_image_path}' style='width: 250px; height: 180px; object-fit: cover; border-radius: 8px; display: block; margin: auto;'/>\";\n",
    "        }});\n",
    "}}\n",
    "</script>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in case you haven't run the code in order, make sure to run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_ward = AgglomerativeClustering(n_clusters=100, linkage='ward').fit(data_sample[[\"lat\", \"long\"]] )\n",
    "labels = clustering_ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[45.762611, 4.832805], zoom_start=14)\n",
    "clustered_points = data_sample.assign(cluster=labels)\n",
    "\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(fetch_image_script))\n",
    "\n",
    "for cluster_id in clustered_points['cluster'].unique():\n",
    "    cluster_points = clustered_points[clustered_points['cluster'] == cluster_id]\n",
    "    points = cluster_points[['lat', 'long']].values\n",
    "\n",
    "    if len(points) >= 10: # We only show clusters with more than 10 points.\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]\n",
    "            \n",
    "            folium.Polygon(\n",
    "                locations=hull_points,\n",
    "                color=cluster_colors[cluster_id % len(cluster_colors)], \n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_opacity=0.2,\n",
    "            ).add_to(m)\n",
    "        except QhullError:\n",
    "            print(f\"Cluster {cluster_id}: QhullError occurred; skipping hull computation.\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}: Less than 10 points; skipping perimeter.\")\n",
    "\n",
    "\n",
    "for i, row in data_sample.iterrows():\n",
    "    lat, lon, photo_id = row[\"lat\"], row[\"long\"], row[\"id\"]\n",
    "    \n",
    "    \n",
    "    link_id = f\"image_link_{i}\"\n",
    "    container_id = f\"image_container_{i}\"\n",
    "\n",
    "    popup_html = f\"\"\"\n",
    "    <div id=\"{container_id}\" style=\"width: 250px; height: 180px; display: flex; align-items: center; justify-content: center; text-align: center; border-radius: 8px; background-color: white; padding: 10px;\">\n",
    "        <a href=\"#\" id=\"{link_id}\" onclick=\"fetchFlickrImage('{photo_id}', '{link_id}', '{container_id}'); return false;\"\n",
    "           style=\"text-decoration: none; font-size: 14px; color: blue; font-weight: bold;\">\n",
    "            📷 Click to Load Image\n",
    "        </a>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    popup = folium.Popup(popup_html, max_width=270)\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        icon=folium.DivIcon(html=f\"\"\"\n",
    "        <svg width=\"10px\" height=\"10px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "            <circle cx=\"12\" cy=\"12\" r=\"5\" fill=\"black\"/>\n",
    "        </svg>\n",
    "        \"\"\"),\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save(\"cluster_map_click_fixed_size_image.html\")\n",
    "m\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
